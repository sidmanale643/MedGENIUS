{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "medGENIUS",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers trl datasets accelerate peft bitsandbytes"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-09T12:29:39.375302Z",
          "iopub.execute_input": "2024-04-09T12:29:39.37573Z",
          "iopub.status.idle": "2024-04-09T12:29:58.986389Z",
          "shell.execute_reply.started": "2024-04-09T12:29:39.375696Z",
          "shell.execute_reply": "2024-04-09T12:29:58.985241Z"
        },
        "trusted": true,
        "id": "v3DemiC20UVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer , AutoModelForCausalLM , BitsAndBytesConfig\n",
        "import torch\n",
        "from datasets import Dataset , load_dataset\n",
        "from peft import LoraConfig , prepare_model_for_kbit_training , AutoPeftModelForCausalLM , get_peft_model\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:32:04.834499Z",
          "iopub.execute_input": "2024-04-09T12:32:04.834887Z",
          "iopub.status.idle": "2024-04-09T12:32:10.171409Z",
          "shell.execute_reply.started": "2024-04-09T12:32:04.834856Z",
          "shell.execute_reply": "2024-04-09T12:32:10.17037Z"
        },
        "trusted": true,
        "id": "c7WqO_mI0UVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "VhK1Cz1Y0UVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype = torch.float16)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:32:14.37566Z",
          "iopub.execute_input": "2024-04-09T12:32:14.376963Z",
          "iopub.status.idle": "2024-04-09T12:32:14.382207Z",
          "shell.execute_reply.started": "2024-04-09T12:32:14.376929Z",
          "shell.execute_reply": "2024-04-09T12:32:14.381379Z"
        },
        "trusted": true,
        "id": "eYci9RF90UVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id , quantization_config = bnb_config , device_map = \"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id ,padding_side = 'left' , add_bos_token = True ,  add_eos_token = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:32:16.111604Z",
          "iopub.execute_input": "2024-04-09T12:32:16.112488Z",
          "iopub.status.idle": "2024-04-09T12:34:22.5432Z",
          "shell.execute_reply.started": "2024-04-09T12:32:16.112451Z",
          "shell.execute_reply": "2024-04-09T12:34:22.542188Z"
        },
        "trusted": true,
        "id": "wy8lyl9g0UVJ",
        "outputId": "ab1cc571-a59c-48b1-cbf7-c508dd5eb01e",
        "colab": {
          "referenced_widgets": [
            "95d5fb5a73034015aed050ae27f2c168",
            "2ce46108f80a40a3a9f13f50e256e40c",
            "7e750446c953472498913ca3f0dcc14c",
            "288589f342564782a0c564d74111e542",
            "959d0a21a24a4a05beb14c9f7e078fb6",
            "27e9f5af087c46e6b553174c3c835595",
            "a172fe845b334d77869094ca03e27ed7",
            "8fd1eed076944b9eb52ebdac2c2572ef",
            "d4305d5974b44c3190ace1eb90122531",
            "d45189f322d54fdd89e6a1e805ca408d",
            "69e6b0dc907c4ef5b299a30730244a97",
            "f39f367344e745d1ad7b9b30b837b414"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95d5fb5a73034015aed050ae27f2c168"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ce46108f80a40a3a9f13f50e256e40c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e750446c953472498913ca3f0dcc14c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "288589f342564782a0c564d74111e542"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "959d0a21a24a4a05beb14c9f7e078fb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e9f5af087c46e6b553174c3c835595"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a172fe845b334d77869094ca03e27ed7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd1eed076944b9eb52ebdac2c2572ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4305d5974b44c3190ace1eb90122531"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d45189f322d54fdd89e6a1e805ca408d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69e6b0dc907c4ef5b299a30730244a97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f39f367344e745d1ad7b9b30b837b414"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:34:27.586125Z",
          "iopub.execute_input": "2024-04-09T12:34:27.586519Z",
          "iopub.status.idle": "2024-04-09T12:34:27.591764Z",
          "shell.execute_reply.started": "2024-04-09T12:34:27.586486Z",
          "shell.execute_reply": "2024-04-09T12:34:27.590772Z"
        },
        "trusted": true,
        "id": "yV_BIB100UVK",
        "outputId": "ab1ce877-1c20-4803-cac8-1f81e8b2290a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"LinhDuong/chatdoctor-5k\"\n",
        "dataset = load_dataset(data_path , split = 'train')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:34:30.183573Z",
          "iopub.execute_input": "2024-04-09T12:34:30.18428Z",
          "iopub.status.idle": "2024-04-09T12:34:50.284805Z",
          "shell.execute_reply.started": "2024-04-09T12:34:30.184249Z",
          "shell.execute_reply": "2024-04-09T12:34:50.28379Z"
        },
        "trusted": true,
        "id": "BJCAEtbd0UVL",
        "outputId": "f75fbb2c-6bbc-40aa-e492-928a2f9ef6e5",
        "colab": {
          "referenced_widgets": [
            "684bce5d8d934600a76acebc95f516ae",
            "46009622d3a54ca8bb473b7c8cb1ff1a"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/271 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "684bce5d8d934600a76acebc95f516ae"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Downloading data: 100%|██████████| 3.05M/3.05M [00:02<00:00, 1.02MB/s]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46009622d3a54ca8bb473b7c8cb1ff1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:34:52.769214Z",
          "iopub.execute_input": "2024-04-09T12:34:52.769596Z",
          "iopub.status.idle": "2024-04-09T12:34:52.78158Z",
          "shell.execute_reply.started": "2024-04-09T12:34:52.769566Z",
          "shell.execute_reply": "2024-04-09T12:34:52.780705Z"
        },
        "trusted": true,
        "id": "guaMXTRV0UVL",
        "outputId": "4402becd-df99-4fa3-e87d-342abe69ffee"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'output': \"Well, based on what you're telling me, it sounds like you may be suffering from panic disorder. The best course of action is to start with psychotherapy and mental health counseling. Additionally, we should conduct an electrocardiogram to make sure that there are no physical issues causing your panic attacks. We will also need to perform a depression screen and a toxicology screen to rule out any other underlying causes. Finally, I would recommend a comprehensive psychological and psychiatric evaluation and therapy to help manage your symptoms.\",\n 'input': \"Doctor, I have been experiencing sudden and frequent panic attacks. I don't know what to do.\",\n 'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\"}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### Instruction: {example['instruction']} Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:35:32.60013Z",
          "iopub.execute_input": "2024-04-09T12:35:32.601005Z",
          "iopub.status.idle": "2024-04-09T12:35:32.605532Z",
          "shell.execute_reply.started": "2024-04-09T12:35:32.60097Z",
          "shell.execute_reply": "2024-04-09T12:35:32.604448Z"
        },
        "trusted": true,
        "id": "c7iw4_pT0UVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    result = tokenizer(formatting_func(prompt),\n",
        "                    truncation = True ,\n",
        "                    padding = \"max_length\",\n",
        "                    max_length = max_length)\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:35:32.877929Z",
          "iopub.execute_input": "2024-04-09T12:35:32.878282Z",
          "iopub.status.idle": "2024-04-09T12:35:32.883718Z",
          "shell.execute_reply.started": "2024-04-09T12:35:32.878239Z",
          "shell.execute_reply": "2024-04-09T12:35:32.88264Z"
        },
        "trusted": true,
        "id": "I53F3Ct20UVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size = 0.2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:37:08.588548Z",
          "iopub.execute_input": "2024-04-09T12:37:08.588906Z",
          "iopub.status.idle": "2024-04-09T12:37:08.616448Z",
          "shell.execute_reply.started": "2024-04-09T12:37:08.588882Z",
          "shell.execute_reply": "2024-04-09T12:37:08.615506Z"
        },
        "trusted": true,
        "id": "d6ngQ9gb0UVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[\"train\"]\n",
        "eval_data = dataset[\"test\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:37:10.889746Z",
          "iopub.execute_input": "2024-04-09T12:37:10.890191Z",
          "iopub.status.idle": "2024-04-09T12:37:10.894683Z",
          "shell.execute_reply.started": "2024-04-09T12:37:10.890156Z",
          "shell.execute_reply": "2024-04-09T12:37:10.893695Z"
        },
        "trusted": true,
        "id": "HWn2WFnI0UVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.map(generate_and_tokenize_prompt)\n",
        "eval_data = eval_data.map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:37:36.853068Z",
          "iopub.execute_input": "2024-04-09T12:37:36.85373Z",
          "iopub.status.idle": "2024-04-09T12:37:42.744601Z",
          "shell.execute_reply.started": "2024-04-09T12:37:36.853695Z",
          "shell.execute_reply": "2024-04-09T12:37:42.743605Z"
        },
        "trusted": true,
        "id": "iHoCijXy0UVS",
        "outputId": "4920e03d-f68c-4276-ab14-0ad27d70dff8",
        "colab": {
          "referenced_widgets": [
            "9a805e94b0264f2794b4864b897f87b0",
            "352cbca99b8249bea810d829d34bff30"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/4361 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a805e94b0264f2794b4864b897f87b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1091 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "352cbca99b8249bea810d829d34bff30"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(train_data, eval_data):\n",
        "    lengths = [len(x['input_ids']) for x in train_data]\n",
        "    lengths += [len(x['input_ids']) for x in eval_data]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(train_data, eval_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:38:27.361206Z",
          "iopub.execute_input": "2024-04-09T12:38:27.36185Z",
          "iopub.status.idle": "2024-04-09T12:38:33.78248Z",
          "shell.execute_reply.started": "2024-04-09T12:38:27.361812Z",
          "shell.execute_reply": "2024-04-09T12:38:33.781518Z"
        },
        "trusted": true,
        "id": "pDaLEYXh0UVS",
        "outputId": "ea039019-0493-4d2c-b773-b72babbc6859"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "5452\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = \" Doctor, I have been experiencing sudden and frequent panic attacks. I don't know what to do.\"\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\" ).to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:38:44.489859Z",
          "iopub.execute_input": "2024-04-09T12:38:44.490221Z",
          "iopub.status.idle": "2024-04-09T12:39:06.735615Z",
          "shell.execute_reply.started": "2024-04-09T12:38:44.490192Z",
          "shell.execute_reply": "2024-04-09T12:39:06.73467Z"
        },
        "trusted": true,
        "id": "pey8kBRG0UVS",
        "outputId": "100475c8-7a54-4c06-d306-654e1ccb8747"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " Doctor, I have been experiencing sudden and frequent panic attacks. I don't know what to do.\n\nI'm sorry to hear that you're going through this. Panic attacks can be very distressing and debilitating. Here are some steps you can take to help manage your symptoms:\n\n1. Practice relaxation techniques: Deep breathing exercises, progressive muscle relaxation, or meditation can help calm your mind and body during a panic attack. Try practicing these techniques when you're feeling calm so that you can remember them during an attack.\n2. Identify triggers: If there are specific situations or things that trigger your panic attacks, try to avoid them if possible. If avoiding the trigger isn't an option, prepare yourself by using relaxation techniques beforehand.\n3. Seek support: Talking to a trusted friend or family member about your anxiety can be helpful. You may also consider seeking professional help from a therapist or counselor who specializes in anxiety disorders.\n4. Stay informed: Educating yourself about panic attacks and anxiety disorders can help reduce fear and anxiety around them. There are many resources available online and in books.\n5. Consider medication: In some cases, medication may be necessary to help manage severe anxiety or panic attacks. Your doctor can help determine if this is an appropriate treatment option for you.\n6\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:41:32.992914Z",
          "iopub.execute_input": "2024-04-09T12:41:32.993805Z",
          "iopub.status.idle": "2024-04-09T12:41:33.008226Z",
          "shell.execute_reply.started": "2024-04-09T12:41:32.993769Z",
          "shell.execute_reply": "2024-04-09T12:41:33.007341Z"
        },
        "trusted": true,
        "id": "_8ontdX-0UVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:40:02.924888Z",
          "iopub.execute_input": "2024-04-09T12:40:02.92553Z",
          "iopub.status.idle": "2024-04-09T12:40:02.931033Z",
          "shell.execute_reply.started": "2024-04-09T12:40:02.925496Z",
          "shell.execute_reply": "2024-04-09T12:40:02.930054Z"
        },
        "trusted": true,
        "id": "Td5R-Yyh0UVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:41:36.27786Z",
          "iopub.execute_input": "2024-04-09T12:41:36.278666Z",
          "iopub.status.idle": "2024-04-09T12:41:36.286999Z",
          "shell.execute_reply.started": "2024-04-09T12:41:36.278633Z",
          "shell.execute_reply": "2024-04-09T12:41:36.286038Z"
        },
        "trusted": true,
        "id": "x3imhEFp0UVT",
        "outputId": "3071b32f-e147-4e9e-ae33-df55bb1452e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 50,
          "output_type": "execute_result",
          "data": {
            "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "r = 32,\n",
        "lora_alpha = 64,\n",
        "target_modules = ['q_proj' , 'k_proj' , 'v_proj' , 'o_proj' , 'gate_proj' , 'up_proj' , 'down_proj' , 'lm_head'],\n",
        "bias = \"none\",\n",
        "lora_dropout = 0.01,\n",
        "task_type = \"CAUSAL_LM\")\n",
        "model = get_peft_model(model , lora_config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:42:26.400194Z",
          "iopub.execute_input": "2024-04-09T12:42:26.401039Z",
          "iopub.status.idle": "2024-04-09T12:42:27.740177Z",
          "shell.execute_reply.started": "2024-04-09T12:42:26.401006Z",
          "shell.execute_reply": "2024-04-09T12:42:27.739206Z"
        },
        "trusted": true,
        "id": "36cZ1mIn0UVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "xM_JVeld0UVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:42:29.90345Z",
          "iopub.execute_input": "2024-04-09T12:42:29.903822Z",
          "iopub.status.idle": "2024-04-09T12:42:29.927995Z",
          "shell.execute_reply.started": "2024-04-09T12:42:29.903792Z",
          "shell.execute_reply": "2024-04-09T12:42:29.92697Z"
        },
        "trusted": true,
        "id": "OyJjxmSM0UVT",
        "outputId": "40384844-b873-4ef5-e037-6b9e62ba0000"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 53,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.01, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=32, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=32, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:42:48.113842Z",
          "iopub.execute_input": "2024-04-09T12:42:48.114922Z",
          "iopub.status.idle": "2024-04-09T12:42:48.119127Z",
          "shell.execute_reply.started": "2024-04-09T12:42:48.114887Z",
          "shell.execute_reply": "2024-04-09T12:42:48.118093Z"
        },
        "trusted": true,
        "id": "u2upztav0UVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "project = \"medgeinus\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,\n",
        "        logging_dir=\"./logs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=25,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=25,\n",
        "        do_eval=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T12:44:36.92032Z",
          "iopub.execute_input": "2024-04-09T12:44:36.920695Z",
          "iopub.status.idle": "2024-04-09T13:58:20.149396Z",
          "shell.execute_reply.started": "2024-04-09T12:44:36.920665Z",
          "shell.execute_reply": "2024-04-09T13:58:20.147403Z"
        },
        "trusted": true,
        "id": "MAn3ermt0UVT",
        "outputId": "868b50ae-b851-4d73-f491-df3bcd5ef110"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240409_124504-ujr3xb1z</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/sidmanale643/huggingface/runs/ujr3xb1z/workspace' target=\"_blank\">mistral-medgeinus-2024-04-09-12-44</a></strong> to <a href='https://wandb.ai/sidmanale643/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/sidmanale643/huggingface' target=\"_blank\">https://wandb.ai/sidmanale643/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/sidmanale643/huggingface/runs/ujr3xb1z/workspace' target=\"_blank\">https://wandb.ai/sidmanale643/huggingface/runs/ujr3xb1z/workspace</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='176' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [176/500 1:08:38 < 2:07:49, 0.04 it/s, Epoch 0.08/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.451400</td>\n      <td>1.094468</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.047300</td>\n      <td>1.016323</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.036800</td>\n      <td>0.959948</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.989800</td>\n      <td>0.913298</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.889000</td>\n      <td>0.890241</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.906600</td>\n      <td>0.876652</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='61' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 61/137 04:09 < 05:15, 0.24 it/s]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:155\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[57], line 37\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2577\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2577\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3365\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3362\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3364\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3365\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3375\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3544\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3542\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3543\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3544\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3545\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:461\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:157\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T13:59:13.753042Z",
          "iopub.execute_input": "2024-04-09T13:59:13.75344Z",
          "iopub.status.idle": "2024-04-09T14:01:25.605804Z",
          "shell.execute_reply.started": "2024-04-09T13:59:13.753408Z",
          "shell.execute_reply": "2024-04-09T14:01:25.604744Z"
        },
        "trusted": true,
        "id": "kj7YuF1f0UVU",
        "outputId": "7a4aea19-2acc-42fc-8460-28ef74a0bafd",
        "colab": {
          "referenced_widgets": [
            "a927c79ebcf949faac353a842c379019",
            "83176a6dbe7f4f7cb6006f8369ab877d",
            "281844e1844741f69e6309ef0629c6ad",
            "89d800adaade412db8335097cfce309d",
            "691c3f5e456045d8bfa126c932bf9cee",
            "f93df2068fdc40a5ae75222e0b0883f0",
            "badd14a1580943ee95c30bf3c273ae54",
            "78159787bb724b0684417e15c2bf1e25",
            "61ee39d250c14d8da1beffd4886248f3",
            "d7af5f5bb2a34b1f95862c01e3ab0070",
            "65fddd7cb1c34abfb2dc76775cf510da"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a927c79ebcf949faac353a842c379019"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83176a6dbe7f4f7cb6006f8369ab877d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "281844e1844741f69e6309ef0629c6ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89d800adaade412db8335097cfce309d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "691c3f5e456045d8bfa126c932bf9cee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f93df2068fdc40a5ae75222e0b0883f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "badd14a1580943ee95c30bf3c273ae54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78159787bb724b0684417e15c2bf1e25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ee39d250c14d8da1beffd4886248f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7af5f5bb2a34b1f95862c01e3ab0070"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fddd7cb1c34abfb2dc76775cf510da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/mistral-medgeinus/checkpoint-125\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:01:38.624023Z",
          "iopub.execute_input": "2024-04-09T14:01:38.624919Z",
          "iopub.status.idle": "2024-04-09T14:01:43.830633Z",
          "shell.execute_reply.started": "2024-04-09T14:01:38.624886Z",
          "shell.execute_reply": "2024-04-09T14:01:43.829247Z"
        },
        "trusted": true,
        "id": "ZPmMyamB0UVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:02:02.97014Z",
          "iopub.execute_input": "2024-04-09T14:02:02.971194Z",
          "iopub.status.idle": "2024-04-09T14:02:02.996022Z",
          "shell.execute_reply.started": "2024-04-09T14:02:02.971146Z",
          "shell.execute_reply": "2024-04-09T14:02:02.995174Z"
        },
        "trusted": true,
        "id": "QfzCBCUz0UVU",
        "outputId": "62497326-0b1a-4f25-f53a-f3215ed0c115"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 60,
          "output_type": "execute_result",
          "data": {
            "text/plain": "PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.01, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): lora.Linear(\n        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n        (lora_dropout): ModuleDict(\n          (default): Dropout(p=0.01, inplace=False)\n        )\n        (lora_A): ModuleDict(\n          (default): Linear(in_features=4096, out_features=32, bias=False)\n        )\n        (lora_B): ModuleDict(\n          (default): Linear(in_features=32, out_features=32000, bias=False)\n        )\n        (lora_embedding_A): ParameterDict()\n        (lora_embedding_B): ParameterDict()\n      )\n    )\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = train_data[643]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:13:19.059299Z",
          "iopub.execute_input": "2024-04-09T14:13:19.059747Z",
          "iopub.status.idle": "2024-04-09T14:13:19.067958Z",
          "shell.execute_reply.started": "2024-04-09T14:13:19.059716Z",
          "shell.execute_reply": "2024-04-09T14:13:19.066831Z"
        },
        "trusted": true,
        "id": "mYmtrgiV0UVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt['instruction'] + eval_prompt['input']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:13:50.904097Z",
          "iopub.execute_input": "2024-04-09T14:13:50.904776Z",
          "iopub.status.idle": "2024-04-09T14:13:50.912811Z",
          "shell.execute_reply.started": "2024-04-09T14:13:50.90474Z",
          "shell.execute_reply": "2024-04-09T14:13:50.911807Z"
        },
        "trusted": true,
        "id": "G65frS7X0UVU",
        "outputId": "4a9f5a5c-4f5c-46d2-b39d-3b629cd9db74"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 77,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"If you are a doctor, please answer the medical questions based on the patient's description.Doctor, I have been experiencing some unusual symptoms lately. My shoulder is swollen, I have been having seizures, and my lips are swollen. Additionally, I have been biting my nails more often, and I am experiencing some hip stiffness or tightness.\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = \"Doctor, I have been having some trouble lately with my sleeping patterns. I often feel tired during the day, and even though I try to get enough sleep at night. ### Answer:\"\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:17:16.483616Z",
          "iopub.execute_input": "2024-04-09T14:17:16.484029Z",
          "iopub.status.idle": "2024-04-09T14:17:30.873367Z",
          "shell.execute_reply.started": "2024-04-09T14:17:16.483995Z",
          "shell.execute_reply": "2024-04-09T14:17:30.872305Z"
        },
        "trusted": true,
        "id": "I3l1zNvZ0UVU",
        "outputId": "bc8fc854-e7fc-4859-857d-3f21a43efbb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Doctor, I have been having some trouble lately with my sleeping patterns. I often feel tired during the day, and even though I try to get enough sleep at night. ### Answer:\n ### 1. The patient is suffering from a condition called insomnia. It is a common problem that affects many people in their daily lives.\n### 2. Insomnia can be caused by various factors such as stress, anxiety, depression or other medical conditions like heart disease or high blood pressure.\n### 3. To treat this condition effectively it's important for patients to understand what causes their symptoms so they can take steps towards managing them better over time.\n###\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:18:38.589782Z",
          "iopub.execute_input": "2024-04-09T14:18:38.590552Z",
          "iopub.status.idle": "2024-04-09T14:18:38.615383Z",
          "shell.execute_reply.started": "2024-04-09T14:18:38.59052Z",
          "shell.execute_reply": "2024-04-09T14:18:38.61455Z"
        },
        "trusted": true,
        "id": "7vUMCU6O0UVU",
        "outputId": "a438d29c-23f4-4c99-cacc-f46cd0a393ff",
        "colab": {
          "referenced_widgets": [
            "e0ac429b19504eaf9415ed17e1ea2dcc"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0ac429b19504eaf9415ed17e1ea2dcc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T14:19:06.63207Z",
          "iopub.execute_input": "2024-04-09T14:19:06.63245Z",
          "iopub.status.idle": "2024-04-09T14:19:39.17852Z",
          "shell.execute_reply.started": "2024-04-09T14:19:06.63242Z",
          "shell.execute_reply": "2024-04-09T14:19:39.177273Z"
        },
        "trusted": true,
        "id": "aqffiZjc0UVU",
        "outputId": "05d38eac-528a-4aa3-a3c9-d7d40aa1e40c",
        "colab": {
          "referenced_widgets": [
            "bfbdbb7d4f51494f857cd67f4b60d1f3",
            "1764afa9da7c424d8e2e3057e843602b",
            "4409b1c380e04b05bd11bf1d1f1026a1"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfbdbb7d4f51494f857cd67f4b60d1f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1764afa9da7c424d8e2e3057e843602b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/865M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4409b1c380e04b05bd11bf1d1f1026a1"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 85,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/sidmanale643/mistral-medgeinus/commit/739eec606ac7d69e146153dcecb7890f87efc9ac', commit_message='End of training', commit_description='', oid='739eec606ac7d69e146153dcecb7890f87efc9ac', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARScm_fB0UVV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}